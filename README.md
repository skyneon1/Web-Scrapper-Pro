# Web Scraper Pro

A full-featured web scraping application with a modern React frontend and FastAPI backend, deployable on Vercel.

https://web-scrapper-pro-eight.vercel.app

## Features

- ğŸ¯ **Dashboard**: Real-time statistics and overview with 3D visualizations
- ğŸ•·ï¸ **Web Scraping**: Extract data from any website
- ğŸ“Š **Analytics**: Visual charts and job statistics
- ğŸ“œ **Job History**: View and manage all scraping jobs with organized data display
- ğŸ“¥ **Export**: Download results as JSON or CSV
- ğŸ¨ **Modern UI**: Material-UI design with responsive layout and dark mode
- âš¡ **Fast API**: Async FastAPI backend with background jobs
- ğŸ” **Advanced Extraction**: Metadata, contact info, social links, and more
- ğŸŒ **Playwright Support**: Scrape JavaScript-heavy websites
- ğŸ—ï¸ **Vercel Ready**: Fully configured for Vercel deployment

## Tech Stack

- **Backend**: FastAPI (Python) with Mangum for Vercel
- **Frontend**: React 18 + Material-UI 5 + React Router
- **Scraping**: requests + BeautifulSoup4, Playwright (for JS pages)
- **Storage**: In-memory storage (SQLite ready)
- **Charts**: Recharts for data visualization
- **Deployment**: Vercel (serverless functions)

## Quick Start

### Local Development

#### Backend

```bash
# Install dependencies
pip install -r requirements.txt

# Run server
python run.py
```

Backend runs at `http://localhost:8000`

#### Frontend

```bash
# Navigate to frontend
cd frontend

# Install dependencies
npm install

# Start dev server
npm start
```

Frontend runs at `http://localhost:3000`

## Vercel Deployment

See [VERCEL_SETUP.md](./VERCEL_SETUP.md) for detailed deployment instructions.

### Quick Deploy

1. **Install Vercel CLI**:
   ```bash
   npm install -g vercel
   ```

2. **Login and Deploy**:
   ```bash
   vercel login
   vercel
   ```

3. **Deploy to Production**:
   ```bash
   vercel --prod
   ```

### Manual Deployment

1. Push code to GitHub
2. Go to [vercel.com](https://vercel.com)
3. Import your repository
4. Configure:
   - **Root Directory**: `./`
   - **Build Command**: `cd frontend && npm install && npm run build`
   - **Output Directory**: `frontend/build`
5. Add environment variables (optional):
   - `REACT_APP_API_URL`: Leave empty for relative paths
   - `ALLOWED_ORIGINS`: Your Vercel domain
6. Deploy!

## Project Structure

```
WebScrapper/
â”œâ”€â”€ api/                 # Vercel serverless functions
â”‚   â”œâ”€â”€ index.py        # Main API handler
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ app/                 # Backend application
â”‚   â”œâ”€â”€ main.py         # FastAPI application
â”‚   â”œâ”€â”€ routes.py       # API routes
â”‚   â”œâ”€â”€ scraper.py      # Scraping logic
â”‚   â”œâ”€â”€ models.py       # Data models
â”‚   â””â”€â”€ database.py    # Database models
â”œâ”€â”€ frontend/            # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/ # React components
â”‚   â”‚   â”œâ”€â”€ pages/      # Page components
â”‚   â”‚   â”œâ”€â”€ services/   # API service
â”‚   â”‚   â””â”€â”€ contexts/   # React contexts
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ results/             # Scraped data storage
â”œâ”€â”€ vercel.json         # Vercel configuration
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ run.py             # Backend startup script
â””â”€â”€ README.md
```

## API Endpoints

- `POST /api/scrape` - Create a new scraping job
- `GET /api/jobs/{job_id}` - Get job details
- `GET /api/jobs` - List all jobs
- `DELETE /api/jobs/{job_id}` - Delete a job
- `GET /api/analytics` - Get analytics and statistics
- `GET /api/export/{job_id}/json` - Export result as JSON
- `GET /api/export/{job_id}/csv` - Export result as CSV
- `GET /api/health` - Health check

View interactive API documentation at `/docs` (when running locally)

## Usage

### Using the Web Interface

1. **Dashboard**: View statistics and overview of all jobs
2. **Scraper**: Create new scraping jobs by entering a URL
3. **History**: View all past jobs, export results, or delete jobs
4. **Analytics**: See visual charts and statistics
5. **Settings**: View application information and configure dark mode

### Creating a Scraping Job

1. Go to the "Scraper" page
2. Enter the URL you want to scrape
3. Optionally enable:
   - **Playwright**: For JavaScript-heavy sites
   - **Site-wide Crawl**: Crawl entire website
   - **Max Pages**: Limit number of pages to crawl
4. Click "Start Scraping"
5. View results in the "History" page

## Features

### Data Extraction

- âœ… Page title and meta tags
- âœ… Open Graph tags
- âœ… Twitter Card tags
- âœ… Structured data (JSON-LD, microdata)
- âœ… Contact information (emails, phones)
- âœ… Social media links
- âœ… All images with details
- âœ… All links with titles
- âœ… Headings (H1, H2, H3)
- âœ… Paragraphs

### Advanced Options

- **Playwright Mode**: For JavaScript-heavy sites
- **Wait Time**: Configurable wait time for Playwright (1-30 seconds)
- **Site Crawling**: Enable to crawl entire site
- **Max Pages**: Control how many pages to crawl (1-50)

## Environment Variables

- `REACT_APP_API_URL`: Frontend API URL (optional, defaults to `/api`)
- `ALLOWED_ORIGINS`: CORS allowed origins (comma-separated)
- `RESULTS_DIR`: Directory for storing results (default: `results`)
- `USER_AGENT`: Custom user agent for scraping
- `REQUEST_TIMEOUT`: Request timeout in seconds (default: 30)
- `MAX_RETRIES`: Maximum retry attempts (default: 3)

## Production Considerations

1. **Database**: Replace in-memory storage with a database (PostgreSQL, MongoDB)
2. **File Storage**: Use cloud storage (AWS S3, Cloudinary) for results
3. **Background Jobs**: Consider using a job queue (Celery, RQ) for long-running tasks
4. **Rate Limiting**: Add rate limiting to prevent abuse
5. **Error Handling**: Improve error handling and logging
6. **Monitoring**: Set up monitoring and alerts
7. **Playwright**: May not work in serverless - consider alternatives for JS-heavy sites

## License

MIT
